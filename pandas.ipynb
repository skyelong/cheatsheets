{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pandas.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "i9IICSBi8cQ0",
        "2mEvczA3LLS3",
        "UzgOiCg7Rybn",
        "-usvGYRiEU7T",
        "wHwkxdyxyJdo",
        "qyEWe3dZ4HGC",
        "qS0F9I3EqqK9",
        "Esxb04_-yPxH",
        "ucT4uOOTNCyL",
        "-ePe-xQ2ITC4",
        "pDMG4iGCNWEi",
        "bj8Kb32VSYrV",
        "OjwodRGkXw4B",
        "M4pMkwa5Keap",
        "DZAyvwze4StN",
        "3V4WVOdqJaR7",
        "W9FKos3GJkhZ",
        "gFa-Vhemu2OL",
        "FzdCE-L7Eu4h",
        "fCZ3QmH-JDRJ",
        "fKhZGqFMKDq7",
        "5A2pl6Rg4bL5",
        "_SWo86_aKnj6",
        "notF1C6U1qZp",
        "VtiSi8Dxqywu",
        "EYKDbygxw-5C",
        "l9HHadaVFI01",
        "F9_8Zn6eFLOw",
        "xMEmunTD_Rxa"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skyelong/cheatsheets/blob/master/pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9IICSBi8cQ0",
        "colab_type": "text"
      },
      "source": [
        "## Skye: Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tKBGXNa8RZI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mEvczA3LLS3",
        "colab_type": "text"
      },
      "source": [
        "#FILTERING ROWS\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2pQeTtk3u49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzgOiCg7Rybn",
        "colab_type": "text"
      },
      "source": [
        "## Skye: PANDAS_Setting a filter to extract data from a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dEoV3S0R2xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select CO2 emissions for the United States\n",
        "hist_indicator = 'CO2 emissions metric' #These are the values for the filter, you can use a form\n",
        "hist_country = 'USA' #This is the filter\n",
        "\n",
        "mask1 = data['IndicatorName'].str.contains(hist_indicator) #Here we apply the filter\n",
        "mask2 = data['CountryCode'].str.contains(hist_country)\n",
        "\n",
        "# stage is just those indicators matching the USA for country code and CO2 emissions over time.\n",
        "stage = data[mask1 & mask2] #Now we use the boolean operator to mask out this data\n",
        "\n",
        "# select CO2 emissions for all countries in 2011\n",
        "hist_indicator = 'CO2 emissions metric'\n",
        "hist_year = 2011 #notice here you use a year this is a number\n",
        "\n",
        "mask1 = data['IndicatorName'].str.contains(hist_indicator) \n",
        "mask2 = data['Year'].isin([hist_year]) #the call is different because it is not a string\n",
        "\n",
        "# apply our mask\n",
        "co2_2011 = data[mask1 & mask2]\n",
        "co2_2011.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-usvGYRiEU7T",
        "colab_type": "text"
      },
      "source": [
        "## SKYE_PANDAS_FILTERING ROWS USING STRING CONDITIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_AcHPaGEa4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Filter out most marching bands\n",
        "no_bands = halftime_musicians[~halftime_musicians.musician.str.contains('Marching')]\n",
        "no_bands = no_bands[~no_bands.musician.str.contains('Spirit')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHwkxdyxyJdo",
        "colab_type": "text"
      },
      "source": [
        "#Renaming columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyEWe3dZ4HGC",
        "colab_type": "text"
      },
      "source": [
        "##Rename_Columns using dict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LytU2FRW4FfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.rename(columns={\"A\": \"a\", \"B\": \"c\"})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS0F9I3EqqK9",
        "colab_type": "text"
      },
      "source": [
        "##Rename multiple columns using a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv7eVCrVquEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create list of new column labels: new_labels\n",
        "new_labels = ['NOC', 'Country', 'Gold']\n",
        "\n",
        "# Rename the columns of medals using new_labels\n",
        "medals.columns = new_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Esxb04_-yPxH",
        "colab_type": "text"
      },
      "source": [
        "##Rename columns using part of a string\n",
        ".str.replace()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGLSS7RdyVxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract selected columns from weather as new DataFrame: temps_f\n",
        "temps_f = weather[['Min TemperatureF','Mean TemperatureF','Max TemperatureF']]\n",
        "\n",
        "# Convert temps_f to celsius: temps_c\n",
        "temps_c = (temps_f-32)* 5/9\n",
        "\n",
        "# Rename 'F' in column names with 'C': temps_c.columns\n",
        "temps_c.columns = temps_c.columns.str.replace('F','C')\n",
        "\n",
        "# Print first 5 rows of temps_c\n",
        "print(temps_c.head())\n",
        "\"\"\"\n",
        "                Min TemperatureC  Mean TemperatureC  Max TemperatureC\n",
        "    Date                                                             \n",
        "    2013-01-01         -6.111111          -2.222222          0.000000\n",
        "    2013-01-02         -8.333333          -6.111111         -3.888889\n",
        "    2013-01-03         -8.888889          -4.444444          0.000000\n",
        "    2013-01-04         -2.777778          -2.222222         -1.111111\n",
        "    2013-01-05         -3.888889          -1.111111          1.111111\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucT4uOOTNCyL",
        "colab_type": "text"
      },
      "source": [
        "#SELECTING COLUMNS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6H4LzhhDJYo",
        "colab_type": "text"
      },
      "source": [
        "##using conditionals to select columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9N8V9JoDIgX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cps_temp = chunk[(chunk.TULINENO == 1) & (chunk.HRYEAR4 >= 2008)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ePe-xQ2ITC4",
        "colab_type": "text"
      },
      "source": [
        "## SKYE_PANDAS_SUBSETTING_SORTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVhcIM8pIWhm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Sort on ...\tSyntax\n",
        "#one column\t\n",
        "df.sort_values(\"breed\")\n",
        "#multiple columns\t\n",
        "df.sort_values([\"breed\", \"weight_kg\"])\n",
        "# Sort homelessness by descending family members\n",
        "homelessness_fam = homelessness.sort_values('family_members', ascending=False)\n",
        "# Sort homelessness by region, then descending family members\n",
        "homelessness_reg_fam = homelessness.sort_values([\"region\", \"family_members\"], ascending=[True, False])\n",
        "\n",
        "#SUBSETTING COLUMNS\n",
        "\n",
        "#To select only \"col_a\" of the DataFrame df, use\n",
        "df[\"col_a\"]\n",
        "#To select \"col_a\" and \"col_b\" of df, use\n",
        "df[[\"col_a\", \"col_b\"]]\n",
        "\n",
        "#SUBSETTING ROWS\n",
        "#There are many ways to subset a DataFrame, perhaps the most common is to use relational operators to return True or False for each row, then pass that inside square brackets.\n",
        "\n",
        "dogs[dogs[\"height_cm\"] > 60]\n",
        "dogs[dogs[\"color\"] == \"tan\"]\n",
        "\n",
        "#You can filter for multiple conditions at once by using the \"logical and\" operator, &.\n",
        "dogs[(dogs[\"height_cm\"] > 60) & (dogs[\"col_b\"] == \"tan\")]\n",
        "\n",
        "#Subsetting rows by categorical variables\n",
        "#Subsetting data based on a categorical variable often involves using the \"or\" operator (|) to select rows from multiple categories. \n",
        "#This can get tedious when you want all states in one of three different regions, for example. \n",
        "#Instead, use the .isin() method, which will allow you to tackle this problem by writing one condition instead of three separate ones.\n",
        "\n",
        "colors = [\"brown\", \"black\", \"tan\"]\n",
        "condition = dogs[\"color\"].isin(colors)\n",
        "dogs[condition]\n",
        "\n",
        "# The Mojave Desert states\n",
        "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
        "\n",
        "# Filter for rows in the Mojave Desert states\n",
        "\n",
        "mojave_homelessness = homelessness[homelessness['state'].isin(canu)]\n",
        "\n",
        "# See the result\n",
        "print(mojave_homelessness)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDMG4iGCNWEi",
        "colab_type": "text"
      },
      "source": [
        "#MISC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8UP7Mw_g_5n",
        "colab_type": "text"
      },
      "source": [
        "##SKYE_PANDAS_list comprehentions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvITeWk7g_Kl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ones = [i for i in test_col if i == 0]\n",
        "assert len(ones) == 9037, \"missing values\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GscGE_lAzDrF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhGTsRqBzEbf",
        "colab_type": "text"
      },
      "source": [
        "## SKYE_PANDAS_take the first two characters of a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sw4875aezLK6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num = [z[:2] for z in dict2]\n",
        "text = [z[3:] for z in dict2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVZl_xsDSS-v",
        "colab_type": "text"
      },
      "source": [
        "##SKYE_Pandas_replace values in pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDIe8KdpSSFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_demo_income.education.replace(to_replace=[31,32,33,34,35,36,37], value=38, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NozCVEb7NjUL",
        "colab_type": "text"
      },
      "source": [
        "#EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuwFKFESG1aC",
        "colab_type": "text"
      },
      "source": [
        "##SKYE_PANDAS_DESCRIPTIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxWOoFN-G7GJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#.head() returns the first few rows (the “head” of the DataFrame).\n",
        "df.head(5)\n",
        "#.info() shows information on each of the columns, such as the data type and number of missing values.\n",
        "df.info()\n",
        "#.shape returns the number of rows and columns of the DataFrame.\n",
        "df.shape\n",
        "#.describe() calculates a few summary statistics for each column.\n",
        "df.describe()\n",
        "#.values: A two-dimensional NumPy array of values.\n",
        "df.values\n",
        "#.columns: An index of columns: the column names.\n",
        "df.columns\n",
        "#.index: An index for the rows: either row numbers or row names.\n",
        "df.index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX5nHEZ8U2ke",
        "colab_type": "text"
      },
      "source": [
        "##SKYE_PANDAS_SUMMARY_STATISTICS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oVvKBV1U64a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#SUMMARY ON COLUMNS\n",
        "# Print the mean of weekly_sales\n",
        "print(sales['weekly_sales'].mean())\n",
        "\n",
        "# Print the median of weekly_sales\n",
        "print(sales['weekly_sales'].median())\n",
        "\n",
        "# Print the maximum of the date column\n",
        "print(sales[\"date\"].max())\n",
        "\n",
        "# Print the minimum of the date column\n",
        "print(sales[\"date\"].min())\n",
        "\n",
        "# A custom IQR function\n",
        "def iqr(column):\n",
        "    return column.quantile(0.75) - column.quantile(0.25)\n",
        "    \n",
        "# Print IQR of the temperature_c column\n",
        "print(sales['temperature_c'].agg(iqr))\n",
        "\n",
        "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
        "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr,np.median]))\n",
        "\n",
        "#OUTPUT\n",
        "\"\"\"            temperature_c  fuel_price_usd_per_l  unemployment\n",
        "    iqr            16.583                 0.073         0.565\n",
        "    median         16.967                 0.743         8.099\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMcGMob9Xdcm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CUMMULATIVE STATISTICS\n",
        "# Sort sales_1_1 by date\n",
        "sales_1_1 = sales_1_1.sort_values('date')\n",
        "\n",
        "# Get the cumulative sum of weekly_sales, add as v col\n",
        "sales_1_1['cum_weekly_sales'] = sales_1_1['weekly_sales'].cumsum()\n",
        "\n",
        "# Get the cumulative max of weekly_sales, add as cum_max_sales col\n",
        "sales_1_1['cum_max_sales'] = sales_1_1['weekly_sales'].cummax()\n",
        "\n",
        "\n",
        "# See the columns you calculated\n",
        "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba5uTG4iYFUE",
        "colab_type": "text"
      },
      "source": [
        "##SKYE_PANDAS_COUNTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJRP_gRHYKUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#PREPARING THE DATA FOR COUNTING - need to remove duplicates to avoid doubles\n",
        "# Drop duplicate store/type combinations\n",
        "store_types = sales.drop_duplicates(['store', 'type'])\n",
        "print(store_types.head())\n",
        "\n",
        "# Drop duplicate store/department combinations\n",
        "store_depts = sales.drop_duplicates(['store', 'department'])\n",
        "print(store_depts.head())\n",
        "\n",
        "# Subset the rows that are holiday weeks and drop duplicate dates\n",
        "holiday_dates = sales[sales['is_holiday'].isin([True])].drop_duplicates('date')\n",
        "\n",
        "# Print date col of holiday_dates\n",
        "print(holiday_dates['date'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqwFJMiVa43s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#COUNTING VALUES\n",
        "# Count the number of stores of each type\n",
        "store_counts = stores[\"store_type\"].value_counts()\n",
        "print(store_counts)\n",
        "\n",
        "# Get the proportion of stores of each type\n",
        "store_props = stores[\"store_type\"].value_counts(normalize=True)\n",
        "print(store_props)\n",
        "\n",
        "# Count the number of each department number and sort\n",
        "dept_counts_sorted = departments[\"department_num\"].value_counts(sort=True)\n",
        "print(dept_counts_sorted)\n",
        "\n",
        "# Get the proportion of departments of each number and sort\n",
        "dept_props_sorted = departments[\"department_num\"].value_counts(sort=True, normalize=True)\n",
        "print(dept_props_sorted)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYYYEupiOsu4",
        "colab_type": "text"
      },
      "source": [
        "#MAKING NEW COLUMNS - FEATURE ENGINEERING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj8Kb32VSYrV",
        "colab_type": "text"
      },
      "source": [
        "## SKYE_PANDAS_CREATING NEW COLUMNS (FEATURE ENGINEERING)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWpj3b1OSYPv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add total col as sum of individuals and family_members\n",
        "homelessness[\"total\"] = homelessness[\"individuals\"] + homelessness[\"family_members\"]\n",
        "\n",
        "# Add p_individuals col as proportion of individuals\n",
        "homelessness['p_individuals'] = homelessness['individuals']/homelessness['total']\n",
        "\n",
        "# See the result\n",
        "print(homelessness)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjbO9S-dTuRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"Which state has the highest number of homeless individuals per 10,000 people in the state?\"\n",
        "\n",
        "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
        "homelessness[\"indiv_per_10k\"] = 10000 * homelessness['individuals'] / homelessness['state_pop'] \n",
        "\n",
        "# Subset rows for indiv_per_10k greater than 20\n",
        "high_homelessness = homelessness[homelessness['indiv_per_10k'] > 20]\n",
        "\n",
        "# Sort high_homelessness by descending indiv_per_10k\n",
        "high_homelessness_srt = high_homelessness.sort_values('indiv_per_10k', ascending=False)\n",
        "\n",
        "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
        "result = high_homelessness_srt[['state', 'indiv_per_10k']]\n",
        "\n",
        "# See the result\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq1dC72qOOnp",
        "colab_type": "text"
      },
      "source": [
        "#GROUPBY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPJqPUZRdd7y",
        "colab_type": "text"
      },
      "source": [
        "##SKYE_PANDAS_GROUPED_SUMMARY_STATS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz8D_YcYdkk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Group by type; calc total weekly sales\n",
        "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
        "\n",
        "# Get proportion for each type\n",
        "sales_propn_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()/sales_by_type.sum()\n",
        "print(sales_propn_by_type)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9ApcSjOgZcx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#CALULATIONS WITH GROUPBY\n",
        "# From previous step\n",
        "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
        "\n",
        "# Group by type and is_holiday; calc total weekly sales\n",
        "sales_by_type_is_holiday = sales.groupby([\"type\",\"is_holiday\"])[\"weekly_sales\"].sum()\n",
        "print(sales_by_type_is_holiday)\n",
        "\n",
        "#OUTPUT\n",
        "\"\"\"type  is_holiday     SUM\n",
        "    A     False         2.337e+08\n",
        "          True          2.360e+04\n",
        "    B     False         2.318e+07\n",
        "          True          1.621e+03\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hPab4PbikUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#multiple grouped summaries\n",
        "\"\"\"Earlier in this chapter you saw that the .agg() method is useful to compute multiple statistics on multiple variables. \n",
        "It also works with grouped data. NumPy, which is imported as np, has many different summary statistics functions, including:\n",
        "\n",
        "np.min()\n",
        "np.max()\n",
        "np.mean()\n",
        "np.median()\"\"\"\n",
        "# Import NumPy with the alias np\n",
        "import numpy as np\n",
        "\n",
        "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
        "sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([min,max,np.mean,np.median])\n",
        "\n",
        "# Print sales_stats\n",
        "print(sales_stats)\n",
        "\n",
        "\"\"\"     min        max       mean    median\n",
        "type                                        \n",
        "A    -1098.0  293966.05  23674.667  11943.92\n",
        "B     -798.0  232558.51  25696.678  13336.08\"\"\"\n",
        "\n",
        "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
        "unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([min,max,np.mean,np.median])\n",
        "\n",
        "# Print unemp_fuel_stats\n",
        "print(unemp_fuel_stats)\n",
        "\"\"\"\n",
        "     unemployment                      fuel_price_usd_per_l                     \n",
        "              min    max   mean median                  min    max   mean median\n",
        "type                                                                            \n",
        "A           3.879  8.992  7.973  8.067                0.664  1.107  0.745  0.735\n",
        "B           7.170  9.765  9.279  9.199                0.760  1.108  0.806  0.803\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8wF_PNZo0D5",
        "colab_type": "text"
      },
      "source": [
        "## SKYE MATPLOTLIB PLOTTING USING GROUPS\n",
        "Bonus - how to show a google image\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYDtULg5o6xE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Look at the first few rows of data\n",
        "print(avocados.head())\n",
        "\n",
        "# Get the total number of avocados sold of each size\n",
        "nb_sold_by_size = avocados.groupby(\"size\")[\"nb_sold\"].sum()\n",
        "print(nb_sold_by_size)\n",
        "\"\"\"\n",
        "    size\n",
        "    extra_large    1.562e+08\n",
        "    large          2.015e+09\n",
        "    small          2.055e+09\n",
        "    Name: nb_sold, dtype: float64\"\"\"\n",
        "# Create a bar plot of the number of avocados sold by size\n",
        "nb_sold_by_size.plot(kind=\"bar\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkNaKkK_pVlp",
        "colab_type": "text"
      },
      "source": [
        "![avacado](https://drive.google.com/uc?id=1PVx47lqDzlpwULnKhl-USudRucYAP_KA)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1spIZVX0qxYM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the total number of avocados sold on each date\n",
        "nb_sold_by_date = avocados.groupby(\"date\")[\"nb_sold\"].sum()\n",
        "print(nb_sold_by_date)\n",
        "\"\"\"\n",
        "    date\n",
        "    2015-01-04    2.728e+07\n",
        "    2015-01-11    2.508e+07\n",
        "    2015-01-18    2.496e+07\n",
        "    2015-01-25    2.409e+07\n",
        "    2015-02-01    3.984e+07\n",
        "                    ...    \n",
        "    2018-02-25    2.543e+07\n",
        "    2018-03-04    2.683e+07\n",
        "    2018-03-11    2.609e+07\n",
        "    2018-03-18    2.603e+07\n",
        "    2018-03-25    2.748e+07\n",
        "    Name: nb_sold, Length: 169, dtype: float64\"\"\"\n",
        "# Create a line plot of the number of avocados sold by date\n",
        "nb_sold_by_date.plot(kind=\"line\")\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt6qsI_tq_bH",
        "colab_type": "text"
      },
      "source": [
        "![sales over time](https://drive.google.com/uc?id=1EEXGDxQ4xl-vV38I2jSsMhYIPC-29APe)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPZKO47-rymW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scatter plot of nb_sold vs avg_price with title\n",
        "avocados.plot(x=\"nb_sold\", y=\"avg_price\", kind=\"scatter\", title=\"Number of avocados sold vs. average price\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jWbGtLCr5a2",
        "colab_type": "text"
      },
      "source": [
        "![scatter](https://drive.google.com/uc?id=1h6rZAXwdwuxQxh2OAuTIneRa11IVkA4D)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYdiOlGKr41G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modify bins to 20\n",
        "avocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(alpha=0.5, bins=20)\n",
        "\n",
        "# Modify bins to 20\n",
        "avocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(alpha=0.5, bins=20)\n",
        "\n",
        "# Add a legend\n",
        "plt.legend([\"conventional\", \"organic\"])\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dJsygd9uLxr",
        "colab_type": "text"
      },
      "source": [
        "![sidebyside](https://drive.google.com/uc?id=14UBWbVzZpREz3-gLBYIx_TzacoD2p_95)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86zg3N2GOgIR",
        "colab_type": "text"
      },
      "source": [
        "#PIVOT TABLES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Rs5WjTkE9Q",
        "colab_type": "text"
      },
      "source": [
        "##SKYE PANDAS PIVOT TABLES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYDbSKH3kDu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pivot for mean weekly_sales for each store type\n",
        "mean_sales_by_type = sales.pivot_table(values=\"weekly_sales\", index=\"type\")\n",
        "\n",
        "# Print mean_sales_by_type\n",
        "print(mean_sales_by_type)\n",
        "\n",
        "\"\"\"\n",
        "      weekly_sales\n",
        "type              \n",
        "A        23674.667\n",
        "B        25696.678\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4D-0ai-dljj3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import NumPy as np\n",
        "import numpy as np\n",
        "\n",
        "# Pivot for mean and median weekly_sales for each store type\n",
        "mean_med_sales_by_type = sales.pivot_table(values=\"weekly_sales\", index=\"type\", aggfunc=[np.mean,np.median])\n",
        "\n",
        "# Print mean_med_sales_by_type\n",
        "print(mean_med_sales_by_type)\n",
        "\"\"\"\n",
        "             mean       median\n",
        "     weekly_sales weekly_sales\n",
        "type                          \n",
        "A       23674.667     11943.92\n",
        "B       25696.678     13336.08\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdpTwKghmE5y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pivot for mean weekly_sales by store type and holiday \n",
        "mean_sales_by_type_holiday = sales.pivot_table(values=\"weekly_sales\", index=\"type\", columns=\"is_holiday\") #by default it will give you the mean\n",
        "\n",
        "# Print mean_sales_by_type_holiday\n",
        "print(mean_sales_by_type_holiday)\n",
        "\"\"\"\n",
        "is_holiday      False    True \n",
        "type                          \n",
        "A           23768.584  590.045\n",
        "B           25751.981  810.705\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjoM-QmimThq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print mean weekly_sales by department and type; fill missing values with 0\n",
        "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0))\n",
        "\"\"\"\n",
        "type                 A           B\n",
        "department                        \n",
        "1            30961.725   44050.627\n",
        "2            67600.159  112958.527\n",
        "3            17160.003   30580.655\n",
        "4            44285.399   51219.654\n",
        "5            34821.011   63236.875\n",
        "...                ...         ...\n",
        "95          123933.787   77082.103\n",
        "96           21367.043    9528.538\n",
        "97           28471.267    5828.873\n",
        "98           12875.423     217.428\n",
        "99             379.124       0.000\n",
        "\n",
        "[80 rows x 2 columns]\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_uI-XtvnQFA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
        "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0, margins=True)) #margins will give the mean of the columns, which are means of all rows that match the criteria\n",
        "\n",
        "\"\"\"\n",
        "  type                A           B        All\n",
        "    department                                  \n",
        "    1           30961.725   44050.627  32052.467\n",
        "    2           67600.159  112958.527  71380.023\n",
        "    3           17160.003   30580.655  18278.391\n",
        "    4           44285.399   51219.654  44863.254\n",
        "    5           34821.011   63236.875  37189.000\n",
        "    ...               ...         ...        ...\n",
        "    96          21367.043    9528.538  20337.608\n",
        "    97          28471.267    5828.873  26584.401\n",
        "    98          12875.423     217.428  11820.590\n",
        "    99            379.124       0.000    379.124\n",
        "    All         23674.667   25696.678  23843.950\n",
        "    \n",
        "    [81 rows x 3 columns]\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSFdAI75nyEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
        "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", aggfunc=sum, fill_value=0, margins=True))#this will give you the sum of all rows and the sum of the columns\n",
        "\"\"\"\n",
        "type                A          B        All\n",
        "department                                 \n",
        "1           4.087e+06  5.286e+05  4.616e+06\n",
        "2           8.923e+06  1.356e+06  1.028e+07\n",
        "3           2.265e+06  3.670e+05  2.632e+06\n",
        "4           5.846e+06  6.146e+05  6.460e+06\n",
        "5           4.596e+06  7.588e+05  5.355e+06\n",
        "...               ...        ...        ...\n",
        "96          2.692e+06  1.143e+05  2.807e+06\n",
        "97          3.758e+06  6.995e+04  3.828e+06\n",
        "98          1.700e+06  2.609e+03  1.702e+06\n",
        "99          4.663e+04  0.000e+00  4.663e+04\n",
        "All         2.337e+08  2.318e+07  2.569e+08\n",
        "\n",
        "[81 rows x 3 columns]\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAQ18GfaqyYh",
        "colab_type": "text"
      },
      "source": [
        "##SKYE PANDAS PIVOT TABLES_DATA MANIPULATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psA_BA4IsZHW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add a year column to temperatures\n",
        "temperatures[\"year\"] = temperatures[\"date\"].dt.year\n",
        "# Pivot avg_temp_c by country and city vs year\n",
        "temp_by_country_city_vs_year = temperatures.pivot_table(values=\"avg_temp_c\", index=[\"country\",\"city\"], columns=\"year\")\n",
        "\n",
        "# See the result\n",
        "print(temp_by_country_city_vs_year)\n",
        "\"\"\"\n",
        "<script.py> output:\n",
        "    year                              2000    2001    2002    2003    2004  ...    2009    2010    2011    2012    2013\n",
        "    country       city                                                      ...                                        \n",
        "    Afghanistan   Kabul             15.823  15.848  15.715  15.133  16.128  ...  15.093  15.676  15.812  14.510  16.206\n",
        "    Angola        Luanda            24.410  24.427  24.791  24.867  24.216  ...  24.325  24.440  24.151  24.240  24.554\n",
        "    Australia     Melbourne         14.320  14.180  14.076  13.986  13.742  ...  14.647  14.232  14.191  14.269  14.742\n",
        "                  Sydney            17.567  17.855  17.734  17.592  17.870  ...  18.176  17.999  17.713  17.474  18.090\n",
        "    Bangladesh    Dhaka             25.905  25.931  26.095  25.927  26.136  ...  26.536  26.648  25.803  26.284  26.587\n",
        "    ...                                ...     ...     ...     ...     ...  ...     ...     ...     ...     ...     ...\n",
        "    United States Chicago           11.090  11.703  11.532  10.482  10.943  ...  10.298  11.816  11.214  12.821  11.587\n",
        "                  Los Angeles       16.643  16.466  16.430  16.945  16.553  ...  16.677  15.887  15.875  17.090  18.121\n",
        "                  New York           9.969  10.931  11.252   9.836  10.389  ...  10.142  11.358  11.272  11.972  12.164\n",
        "    Vietnam       Ho Chi Minh City  27.589  27.832  28.065  27.828  27.687  ...  27.853  28.282  27.675  28.249  28.455\n",
        "    Zimbabwe      Harare            20.284  20.861  21.079  20.889  20.308  ...  20.524  21.166  20.782  20.523  19.756\n",
        "    \n",
        "    [100 rows x 14 columns]\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJEnJ3rSifzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subset for Egypt to India\n",
        "temp_by_country_city_vs_year.loc[\"Egypt\":\"India\"]\n",
        "\"\"\"\n",
        "year                    2000    2001    2002    2003    2004  ...    2009    2010    2011    2012    2013\n",
        "country  city                                                 ...                                        \n",
        "Egypt    Alexandria   20.744  21.455  21.456  21.221  21.064  ...  21.671  22.460  21.181  21.553  21.439\n",
        "         Cairo        21.486  22.331  22.414  22.171  22.082  ...  22.625  23.718  21.987  22.484  22.907\n",
        "         Gizeh        21.486  22.331  22.414  22.171  22.082  ...  22.625  23.718  21.987  22.484  22.907\n",
        "Ethiopia Addis Abeba  18.241  18.296  18.470  18.321  18.293  ...  18.765  18.298  18.607  18.449  19.539\n",
        "France   Paris        11.740  11.371  11.871  11.909  11.339  ...  11.464  10.410  12.326  11.220  11.012\n",
        "Germany  Berlin       10.964   9.690  10.264  10.066   9.823  ...  10.062   8.607  10.556   9.964  10.122\n",
        "India    Ahmadabad    27.436  27.198  27.719  27.404  27.628  ...  28.096  28.018  27.290  27.027  27.609\n",
        "         Bangalore    25.338  25.528  25.755  25.925  25.252  ...  25.726  25.705  25.362  26.042  26.611\n",
        "         Bombay       27.204  27.244  27.629  27.578  27.319  ...  27.845  27.765  27.385  27.193  26.713\n",
        "         Calcutta     26.491  26.515  26.704  26.561  26.634  ...  27.153  27.289  26.407  26.935  27.369\n",
        "         Delhi        26.048  25.863  26.634  25.721  26.240  ...  26.554  26.520  25.629  25.889  26.709\n",
        "         Hyderabad    27.232  27.555  27.665  27.845  27.229  ...  28.027  27.693  27.409  28.019  28.851\n",
        "         Jaipur       26.430  26.023  27.032  26.027  26.642  ...  26.919  26.818  25.916  25.885  26.844\n",
        "         Kanpur       25.354  25.326  26.117  25.409  25.587  ...  25.987  26.022  25.062  25.445  26.121\n",
        "         Lakhnau      25.354  25.326  26.117  25.409  25.587  ...  25.987  26.022  25.062  25.445  26.121\n",
        "         Madras       28.812  29.163  29.246  29.273  28.811  ...  29.417  29.047  29.063  29.778  30.412\n",
        "         Nagpur       26.181  26.322  26.753  26.504  26.406  ...  27.139  26.927  26.005  26.328  27.112\n",
        "         New Delhi    26.048  25.863  26.634  25.721  26.240  ...  26.554  26.520  25.629  25.889  26.709\n",
        "         Pune         25.111  25.338  25.583  25.748  25.316  ...  25.868  25.749  25.161  25.297  25.848\n",
        "         Surat        27.029  26.897  27.348  27.231  27.291  ...  27.820  27.682  27.017  26.889  27.438\n",
        "\n",
        "[20 rows x 14 columns]\"\"\"\n",
        "\n",
        "# Subset for Egypt, Cairo to India, Delhi\n",
        "temp_by_country_city_vs_year.loc[(\"Egypt\",\"Cairo\"):(\"India\", \"Delhi\")]\n",
        "\"\"\"\n",
        "year                    2000    2001    2002    2003    2004  ...    2009    2010    2011    2012    2013\n",
        "country  city                                                 ...                                        \n",
        "Egypt    Cairo        21.486  22.331  22.414  22.171  22.082  ...  22.625  23.718  21.987  22.484  22.907\n",
        "         Gizeh        21.486  22.331  22.414  22.171  22.082  ...  22.625  23.718  21.987  22.484  22.907\n",
        "Ethiopia Addis Abeba  18.241  18.296  18.470  18.321  18.293  ...  18.765  18.298  18.607  18.449  19.539\n",
        "France   Paris        11.740  11.371  11.871  11.909  11.339  ...  11.464  10.410  12.326  11.220  11.012\n",
        "Germany  Berlin       10.964   9.690  10.264  10.066   9.823  ...  10.062   8.607  10.556   9.964  10.122\n",
        "India    Ahmadabad    27.436  27.198  27.719  27.404  27.628  ...  28.096  28.018  27.290  27.027  27.609\n",
        "         Bangalore    25.338  25.528  25.755  25.925  25.252  ...  25.726  25.705  25.362  26.042  26.611\n",
        "         Bombay       27.204  27.244  27.629  27.578  27.319  ...  27.845  27.765  27.385  27.193  26.713\n",
        "         Calcutta     26.491  26.515  26.704  26.561  26.634  ...  27.153  27.289  26.407  26.935  27.369\n",
        "         Delhi        26.048  25.863  26.634  25.721  26.240  ...  26.554  26.520  25.629  25.889  26.709\n",
        "\n",
        "[10 rows x 14 columns]\n",
        "\"\"\"\n",
        "# Subset in both directions at once\n",
        "temp_by_country_city_vs_year.loc[(\"Egypt\",\"Cairo\"):(\"India\", \"Delhi\"), \"2005\":\"2010\"]\n",
        "\"\"\"\n",
        "year                    2005    2006    2007    2008    2009    2010\n",
        "country  city                                                       \n",
        "Egypt    Cairo        22.006  22.050  22.361  22.644  22.625  23.718\n",
        "         Gizeh        22.006  22.050  22.361  22.644  22.625  23.718\n",
        "Ethiopia Addis Abeba  18.313  18.427  18.143  18.165  18.765  18.298\n",
        "France   Paris        11.553  11.788  11.751  11.278  11.464  10.410\n",
        "Germany  Berlin        9.919  10.545  10.883  10.658  10.062   8.607\n",
        "India    Ahmadabad    26.828  27.283  27.511  27.049  28.096  28.018\n",
        "         Bangalore    25.476  25.418  25.464  25.353  25.726  25.705\n",
        "         Bombay       27.036  27.381  27.635  27.178  27.845  27.765\n",
        "         Calcutta     26.729  26.986  26.585  26.522  27.153  27.289\n",
        "         Delhi        25.716  26.366  26.146  25.675  26.554  26.520\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loZTnUsjkCO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the worldwide mean temp by year\n",
        "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
        "\"\"\"\n",
        "year\n",
        "2000    19.506\n",
        "2001    19.679\n",
        "2002    19.856\n",
        "2003    19.630\n",
        "2004    19.672\n",
        "2005    19.607\n",
        "2006    19.794\n",
        "2007    19.854\n",
        "2008    19.609\n",
        "2009    19.834\n",
        "2010    19.912\n",
        "2011    19.549\n",
        "2012    19.668\n",
        "2013    20.312\n",
        "dtype: float64\n",
        "\"\"\"\n",
        "# Filter for the year that had the highest mean temp\n",
        "print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
        "\"\"\"\n",
        "year\n",
        "2013    20.312\n",
        "dtype: float64\"\"\"\n",
        "\n",
        "# Get the mean temp by city\n",
        "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=\"columns\")\n",
        "print(mean_temp_by_city)\n",
        "\"\"\"\n",
        "country        city            \n",
        "Afghanistan    Kabul               15.542\n",
        "Angola         Luanda              24.392\n",
        "Australia      Melbourne           14.275\n",
        "               Sydney              17.799\n",
        "Bangladesh     Dhaka               26.174\n",
        "                                    ...  \n",
        "United States  Chicago             11.331\n",
        "               Los Angeles         16.675\n",
        "               New York            10.911\n",
        "Vietnam        Ho Chi Minh City    27.923\n",
        "Zimbabwe       Harare              20.699\n",
        "Length: 100, dtype: float64\n",
        "\"\"\"\n",
        "# Filter for the city that had the lowest mean temp\n",
        "print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])\n",
        "\"\"\"\n",
        "country  city  \n",
        "China    Harbin    4.877\n",
        "dtype: float64\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qlLXKciO4fv",
        "colab_type": "text"
      },
      "source": [
        "#WORKING WITH INDEXES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjwodRGkXw4B",
        "colab_type": "text"
      },
      "source": [
        "## SKYE PANDAS EXPLICIT INDICIES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyGOG8AdXwWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Setting a column as an index\n",
        "# Look at temperatures\n",
        "print(temperatures)\n",
        "\"\"\"\n",
        "               date        country  avg_temp_c\n",
        "    city                                         \n",
        "    Abidjan 2000-01-01  Côte D'Ivoire      27.293\n",
        "    Abidjan 2000-02-01  Côte D'Ivoire      27.685\n",
        "    Abidjan 2000-03-01  Côte D'Ivoire      29.061\n",
        "    Abidjan 2000-04-01  Côte D'Ivoire      28.162\n",
        "    Abidjan 2000-05-01  Côte D'Ivoire      27.547\n",
        "    ...            ...            ...         ...\n",
        "    Xian    2013-05-01          China      18.979\n",
        "    Xian    2013-06-01          China      23.522\n",
        "    Xian    2013-07-01          China      25.251\n",
        "    Xian    2013-08-01          China      24.528\n",
        "    Xian    2013-09-01          China         NaN\n",
        "    \n",
        "    [16500 rows x 3 columns]\n",
        "# Index temperatures by city\"\"\"\n",
        "temperatures_ind = temperatures.set_index(\"city\")\n",
        "\n",
        "# Look at temperatures_ind\n",
        "print(temperatures_ind)\n",
        "\"\"\"\n",
        "               date        country  avg_temp_c\n",
        "    city                                         \n",
        "    Abidjan 2000-01-01  Côte D'Ivoire      27.293\n",
        "    Abidjan 2000-02-01  Côte D'Ivoire      27.685\n",
        "    Abidjan 2000-03-01  Côte D'Ivoire      29.061\n",
        "    Abidjan 2000-04-01  Côte D'Ivoire      28.162\n",
        "    Abidjan 2000-05-01  Côte D'Ivoire      27.547\n",
        "    ...            ...            ...         ...\n",
        "    Xian    2013-05-01          China      18.979\n",
        "    Xian    2013-06-01          China      23.522\n",
        "    Xian    2013-07-01          China      25.251\n",
        "    Xian    2013-08-01          China      24.528\n",
        "    Xian    2013-09-01          China         NaN\n",
        "    \n",
        "    [16500 rows x 3 columns]\"\"\"\n",
        "\n",
        "# Reset the index, keeping its contents\n",
        "print(temperatures_ind.reset_index())\n",
        "\n",
        "# Reset the index, dropping its contents\n",
        "print(temperatures_ind.reset_index(drop=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ92TtNyZcqP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#using loc\n",
        "# Make a list of cities to subset on\n",
        "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
        "\n",
        "# Subset temperatures using square brackets\n",
        "print(temperatures[temperatures[\"city\"].isin(cities)])\n",
        "\n",
        "\"\"\"            date              city country  avg_temp_c\n",
        "10725 2000-01-01            Moscow  Russia      -7.313\n",
        "10726 2000-02-01            Moscow  Russia      -3.551\n",
        "10727 2000-03-01            Moscow  Russia      -1.661\n",
        "10728 2000-04-01            Moscow  Russia      10.096\n",
        "10729 2000-05-01            Moscow  Russia      10.357\n",
        "...          ...               ...     ...         ...\n",
        "13360 2013-05-01  Saint Petersburg  Russia      12.355\n",
        "13361 2013-06-01  Saint Petersburg  Russia      17.185\n",
        "13362 2013-07-01  Saint Petersburg  Russia      17.234\n",
        "13363 2013-08-01  Saint Petersburg  Russia      17.153\n",
        "13364 2013-09-01  Saint Petersburg  Russia         NaN\n",
        "\n",
        "[330 rows x 4 columns]\"\"\"\n",
        "\n",
        "# Subset temperatures_ind using .loc[]\n",
        "print(temperatures_ind.loc[cities])\n",
        "\"\"\"\"\n",
        "city                                           \n",
        "Moscow           2000-01-01  Russia      -7.313\n",
        "Moscow           2000-02-01  Russia      -3.551\n",
        "Moscow           2000-03-01  Russia      -1.661\n",
        "Moscow           2000-04-01  Russia      10.096\n",
        "Moscow           2000-05-01  Russia      10.357\n",
        "...                     ...     ...         ...\n",
        "Saint Petersburg 2013-05-01  Russia      12.355\n",
        "Saint Petersburg 2013-06-01  Russia      17.185\n",
        "Saint Petersburg 2013-07-01  Russia      17.234\n",
        "Saint Petersburg 2013-08-01  Russia      17.153\n",
        "Saint Petersburg 2013-09-01  Russia         NaN\n",
        "\n",
        "[330 rows x 3 columns]\"\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYmnQWkua7z7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Index temperatures by country & city\n",
        "temperatures_ind = temperatures.set_index([\"country\", \"city\"])\n",
        "\n",
        "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
        "rows_to_keep = [(\"Brazil\",\"Rio De Janeiro\"), (\"Pakistan\", \"Lahore\")]\n",
        "\n",
        "# Subset for rows to keep\n",
        "print(temperatures_ind.loc[rows_to_keep])\n",
        "\"\"\"\n",
        "                              date  avg_temp_c\n",
        "country  city                                 \n",
        "Brazil   Rio De Janeiro 2000-01-01      25.974\n",
        "         Rio De Janeiro 2000-02-01      26.699\n",
        "         Rio De Janeiro 2000-03-01      26.270\n",
        "         Rio De Janeiro 2000-04-01      25.750\n",
        "         Rio De Janeiro 2000-05-01      24.356\n",
        "...                            ...         ...\n",
        "Pakistan Lahore         2013-05-01      33.457\n",
        "         Lahore         2013-06-01      34.456\n",
        "         Lahore         2013-07-01      33.279\n",
        "         Lahore         2013-08-01      31.511\n",
        "         Lahore         2013-09-01         NaN\n",
        "\n",
        "[330 rows x 2 columns]\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6juhd8nc3Xh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sort temperatures_ind by index values\n",
        "print(temperatures_ind.sort_index())\n",
        "\"\"\"\n",
        "<script.py> output:\n",
        "                             date  avg_temp_c\n",
        "    country     city                         \n",
        "    Afghanistan Kabul  2000-01-01       3.326\n",
        "                Kabul  2000-02-01       3.454\n",
        "                Kabul  2000-03-01       9.612\n",
        "                Kabul  2000-04-01      17.925\n",
        "                Kabul  2000-05-01      24.658\n",
        "    ...                       ...         ...\n",
        "    Zimbabwe    Harare 2013-05-01      18.298\n",
        "                Harare 2013-06-01      17.020\n",
        "                Harare 2013-07-01      16.299\n",
        "                Harare 2013-08-01      19.232\n",
        "                Harare 2013-09-01         NaN\n",
        "    \n",
        "    [16500 rows x 2 columns]\n",
        "\"\"\"\n",
        "# Sort temperatures_ind by index values at the city level\n",
        "print(temperatures_ind.sort_index(level=[\"city\"]))\n",
        "\"\"\"\n",
        "                            date  avg_temp_c\n",
        "    country       city                          \n",
        "    Côte D'Ivoire Abidjan 2000-01-01      27.293\n",
        "                  Abidjan 2000-02-01      27.685\n",
        "                  Abidjan 2000-03-01      29.061\n",
        "                  Abidjan 2000-04-01      28.162\n",
        "                  Abidjan 2000-05-01      27.547\n",
        "    ...                          ...         ...\n",
        "    China         Xian    2013-05-01      18.979\n",
        "                  Xian    2013-06-01      23.522\n",
        "                  Xian    2013-07-01      25.251\n",
        "                  Xian    2013-08-01      24.528\n",
        "                  Xian    2013-09-01         NaN\n",
        "    \n",
        "    [16500 rows x 2 columns]\"\"\"\n",
        "\n",
        "# Sort temperatures_ind by country then descending city\n",
        "print(temperatures_ind.sort_index(level=[\"country\", \"city\"], ascending=[True,False]))\n",
        "\"\"\"\n",
        "                         date  avg_temp_c\n",
        "    country     city                         \n",
        "    Afghanistan Kabul  2000-01-01       3.326\n",
        "                Kabul  2000-02-01       3.454\n",
        "                Kabul  2000-03-01       9.612\n",
        "                Kabul  2000-04-01      17.925\n",
        "                Kabul  2000-05-01      24.658\n",
        "    ...                       ...         ...\n",
        "    Zimbabwe    Harare 2013-05-01      18.298\n",
        "                Harare 2013-06-01      17.020\n",
        "                Harare 2013-07-01      16.299\n",
        "                Harare 2013-08-01      19.232\n",
        "                Harare 2013-09-01         NaN\n",
        "    \n",
        "    [16500 rows x 2 columns]\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufb21SRxeKOT",
        "colab_type": "text"
      },
      "source": [
        "## SKYE PANDAS SLICING WITH INDEXES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFZcWYZteOMn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sort the index of temperatures_ind\n",
        "temperatures_srt = temperatures_ind.sort_index()\n",
        "\n",
        "# Subset rows from Pakistan to Russia\n",
        "print(temperatures_srt.loc[\"Pakistan\":\"Russia\"])\n",
        "\"\"\"\n",
        "                                  date  avg_temp_c\n",
        "    country  city                                   \n",
        "    Pakistan Faisalabad       2000-01-01      12.792\n",
        "             Faisalabad       2000-02-01      14.339\n",
        "             Faisalabad       2000-03-01      20.309\n",
        "             Faisalabad       2000-04-01      29.072\n",
        "             Faisalabad       2000-05-01      34.845\n",
        "    ...                              ...         ...\n",
        "    Russia   Saint Petersburg 2013-05-01      12.355\n",
        "             Saint Petersburg 2013-06-01      17.185\n",
        "             Saint Petersburg 2013-07-01      17.234\n",
        "             Saint Petersburg 2013-08-01      17.153\n",
        "             Saint Petersburg 2013-09-01         NaN\n",
        "    \n",
        "    [1155 rows x 2 columns]\"\"\"\n",
        "\n",
        "# Subset rows from Pakistan, Lahore to Russia, Moscow\n",
        "print(temperatures_srt.loc[(\"Pakistan\",\"Lahore\"):(\"Russia\", \"Moscow\")])\n",
        "\"\"\"\n",
        "                                  date  avg_temp_c\n",
        "    country  city                                   \n",
        "    Pakistan Faisalabad       2000-01-01      12.792\n",
        "             Faisalabad       2000-02-01      14.339\n",
        "             Faisalabad       2000-03-01      20.309\n",
        "             Faisalabad       2000-04-01      29.072\n",
        "             Faisalabad       2000-05-01      34.845\n",
        "    ...                              ...         ...\n",
        "    Russia   Saint Petersburg 2013-05-01      12.355\n",
        "             Saint Petersburg 2013-06-01      17.185\n",
        "             Saint Petersburg 2013-07-01      17.234\n",
        "             Saint Petersburg 2013-08-01      17.153\n",
        "             Saint Petersburg 2013-09-01         NaN\n",
        "    \n",
        "    [1155 rows x 2 columns]\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfBC05ItRA7h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tp2jQqMggUr7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
        "print(temperatures_srt.loc[(\"India\",\"Hyderabad\"):(\"Iraq\", \"Baghdad\")])\n",
        "\n",
        "\"\"\"                          date  avg_temp_c\n",
        "    country city                            \n",
        "    India   Hyderabad 2000-01-01      23.779\n",
        "            Hyderabad 2000-02-01      25.826\n",
        "            Hyderabad 2000-03-01      28.821\n",
        "            Hyderabad 2000-04-01      32.698\n",
        "            Hyderabad 2000-05-01      32.438\n",
        "    ...                      ...         ...\n",
        "    Iraq    Baghdad   2013-05-01      28.673\n",
        "            Baghdad   2013-06-01      33.803\n",
        "            Baghdad   2013-07-01      36.392\n",
        "            Baghdad   2013-08-01      35.463\n",
        "            Baghdad   2013-09-01         NaN\n",
        "    \n",
        "    [2145 rows x 2 columns]\"\"\"\n",
        "# Subset columns from date to avg_temp_c\n",
        "print(temperatures_srt.loc[:,\"date\":\"avg_temp_c\"])\n",
        "\n",
        "\"\"\"\n",
        "                            date  avg_temp_c\n",
        "    country     city                         \n",
        "    Afghanistan Kabul  2000-01-01       3.326\n",
        "                Kabul  2000-02-01       3.454\n",
        "                Kabul  2000-03-01       9.612\n",
        "                Kabul  2000-04-01      17.925\n",
        "                Kabul  2000-05-01      24.658\n",
        "    ...                       ...         ...\n",
        "    Zimbabwe    Harare 2013-05-01      18.298\n",
        "                Harare 2013-06-01      17.020\n",
        "                Harare 2013-07-01      16.299\n",
        "                Harare 2013-08-01      19.232\n",
        "                Harare 2013-09-01         NaN\n",
        "    \n",
        "    [16500 rows x 2 columns]\"\"\"\n",
        "\n",
        "# Subset in both directions at once\n",
        "print(temperatures_srt.loc[(\"India\",\"Hyderabad\"):(\"Iraq\", \"Baghdad\"),\"date\":\"avg_temp_c\"])\n",
        "\"\"\"\n",
        "                          date  avg_temp_c\n",
        "    country city                            \n",
        "    India   Hyderabad 2000-01-01      23.779\n",
        "            Hyderabad 2000-02-01      25.826\n",
        "            Hyderabad 2000-03-01      28.821\n",
        "            Hyderabad 2000-04-01      32.698\n",
        "            Hyderabad 2000-05-01      32.438\n",
        "    ...                      ...         ...\n",
        "    Iraq    Baghdad   2013-05-01      28.673\n",
        "            Baghdad   2013-06-01      33.803\n",
        "            Baghdad   2013-07-01      36.392\n",
        "            Baghdad   2013-08-01      35.463\n",
        "            Baghdad   2013-09-01         NaN\n",
        "    \n",
        "    [2145 rows x 2 columns]\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Es9ATQVHRQgx"
      },
      "source": [
        "#Missing Data\n",
        "##pandas and matplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNgYBYWBSsMY",
        "colab_type": "text"
      },
      "source": [
        "## .ISNA() / .ISNA().ANY()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F_Ry0IkeRQg1",
        "colab": {}
      },
      "source": [
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check individual values for missing values\n",
        "print(avocados_2016.isna())\n",
        "\"\"\"\n",
        "        date  avg_price  total_sold  small_sold  large_sold  xl_sold  total_bags_sold  small_bags_sold  large_bags_sold  xl_bags_sold\n",
        "    0   False      False       False       False       False    False            False            False            False         False\n",
        "    1   False      False       False       False       False    False            False            False            False         False\n",
        "    2   False      False       False       False        True    False            False            False            False         False\n",
        "    3   False      False       False       False       False    False            False            False            False         False\n",
        "    4   False      False       False       False       False     True            False            False            False         False\n",
        "    5   False      False       False        True       False    False            False            False            False         False\n",
        "    6   False      False       False       False       False    False            False            False            False         False\n",
        "    7   False      False       False       False        True    False            False            False            False         False\n",
        "    8   False      False       False       False       False    False            False            False            False         False\n",
        "    9   False      False       False       False       False    False            False            False            False         False\n",
        "    10  False      False       False       False        True    False            False            False            False         False\n",
        "    11  False      False       False       False       False    False            False            False            False         False\n",
        "    12  False      False       False       False       False    False            False            False            False         False\n",
        "    13  False      False       False       False       False    False            False            False            False         False\n",
        "    14  False      False       False       False       False    False            False            False            False         False\n",
        "\n",
        "\"\"\"\n",
        "# Check each column for missing values\n",
        "print(avocados_2016.isna().any())\n",
        "\"\"\"\n",
        "  date               False\n",
        "    avg_price          False\n",
        "    total_sold         False\n",
        "    small_sold          True\n",
        "    large_sold          True\n",
        "    xl_sold             True\n",
        "    total_bags_sold    False\n",
        "    small_bags_sold    False\n",
        "    large_bags_sold    False\n",
        "    xl_bags_sold       False\n",
        "    dtype: bool\n",
        "\"\"\"\n",
        "\n",
        "# Bar plot of missing values by variable\n",
        "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uNM5jhm9RQg7"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1zmy5LsZR9xuyjcjZsZnH4L2R5l0aXtHM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWb8ILf1S1He",
        "colab_type": "text"
      },
      "source": [
        "##DROPNA()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PhKd88JERQg8",
        "colab": {}
      },
      "source": [
        "# Remove rows with missing values\n",
        "avocados_complete = avocados_2016.dropna()\n",
        "\n",
        "# Check if any columns contain missing values\n",
        "print(avocados_complete.isna().any())\n",
        "\"\"\"\n",
        "    date               False\n",
        "    avg_price          False\n",
        "    total_sold         False\n",
        "    small_sold         False\n",
        "    large_sold         False\n",
        "    xl_sold            False\n",
        "    total_bags_sold    False\n",
        "    small_bags_sold    False\n",
        "    large_bags_sold    False\n",
        "    xl_bags_sold       False\n",
        "    dtype: bool\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k2l2pBcXRQhB",
        "colab": {}
      },
      "source": [
        "# From previous step\n",
        "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
        "avocados_2016[cols_with_missing].hist()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9aBmSxRYRQhG"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1bT7gw0XrzL6U7yGz1Y7_fX84Jmlkg5RB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MvgXzAKS48c",
        "colab_type": "text"
      },
      "source": [
        "#FILLNA()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YgP1SphORQhH",
        "colab": {}
      },
      "source": [
        "# Fill in missing values with 0\n",
        "avocados_filled = avocados_2016.fillna(0)\n",
        "\n",
        "# Create histograms of the filled columns\n",
        "avocados_filled[cols_with_missing].hist()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vH9mJYpNRQhL"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=11m6WBoCsAqmZQWhyQKrgw4zUEsl25_fW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4pMkwa5Keap",
        "colab_type": "text"
      },
      "source": [
        "#ADDING/REMOVING COLUMNS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smq6jskDq1kS",
        "colab_type": "text"
      },
      "source": [
        "##add columns from one df to another df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKGPB_y4q4xq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add columns 'Silver' & 'Bronze' to medals\n",
        "medals['Silver'] = silver[\"Total\"]\n",
        "medals['Bronze'] = bronze[\"Total\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG0ecupOG6RI",
        "colab_type": "text"
      },
      "source": [
        "##Add a new column and fill with a value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5XmiauPG_uT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add 'year' column to names_1881 and names_1981\n",
        "names_1881['year'] = 1881\n",
        "names_1981['year'] = 1981\n",
        "\"\"\"\n",
        "             name gender  count  year\n",
        "    1283   Morgan      M     23  1881\n",
        "    2096   Morgan      F   1769  1981\n",
        "    14390  Morgan      M    766  1981\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZAyvwze4StN",
        "colab_type": "text"
      },
      "source": [
        "##SKYE_PANDAS_DROP_COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eyhu3WDR4WZ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "summ_final.drop(['GEMETSTA'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V4WVOdqJaR7",
        "colab_type": "text"
      },
      "source": [
        "#Reindexing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2OHSvOOtY9g",
        "colab_type": "text"
      },
      "source": [
        "##Reindex using a list and the .ffill() method\n",
        ".ffill() will fill the list with the preceding information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHjoGQk_tYJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "print(year)\n",
        "\"\"\"\n",
        "    ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\"\"\"\n",
        "print(weather1)\n",
        "\"\"\"\n",
        "        Mean TemperatureF\n",
        "    Month                   \n",
        "    Apr            61.956044\n",
        "    Jan            32.133333\n",
        "    Jul            68.934783\n",
        "    Oct            43.434783\"\"\"\n",
        "# Reindex weather1 using the list year: weather2\n",
        "weather2 = weather1.reindex(year)\n",
        "\"\"\"\n",
        "          Mean TemperatureF\n",
        "    Month                   \n",
        "    Jan            32.133333\n",
        "    Feb                  NaN\n",
        "    Mar                  NaN\n",
        "    Apr            61.956044\n",
        "    May                  NaN\n",
        "    Jun                  NaN\n",
        "    Jul            68.934783\n",
        "    Aug                  NaN\n",
        "    Sep                  NaN\n",
        "    Oct            43.434783\n",
        "    Nov                  NaN\n",
        "    Dec                  NaN\"\"\"\n",
        "# Print weather2\n",
        "print(weather2)\n",
        "\n",
        "# Reindex weather1 using the list year with forward-fill: weather3\n",
        "weather3 = weather1.reindex(year).ffill()\n",
        "\n",
        "# Print weather3\n",
        "print(weather3)\n",
        "\"\"\"\n",
        " Mean TemperatureF\n",
        "    Month                   \n",
        "    Jan            32.133333\n",
        "    Feb            32.133333\n",
        "    Mar            32.133333\n",
        "    Apr            61.956044\n",
        "    May            61.956044\n",
        "    Jun            61.956044\n",
        "    Jul            68.934783\n",
        "    Aug            68.934783\n",
        "    Sep            68.934783\n",
        "    Oct            43.434783\n",
        "    Nov            43.434783\n",
        "    Dec            43.434783\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hrbpv3cuu3S",
        "colab_type": "text"
      },
      "source": [
        "##Reindexing with another df"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcyEptt9uy0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "#Both have multi-index generated with the following:\n",
        "#names_1981 = pd.read_csv('names1981.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
        "#names_1881 = pd.read_csv('names1881.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
        "print(names_1981.head())\n",
        "print(names_1881.head())\n",
        "# Reindex names_1981 with index of names_1881: common_names\n",
        "common_names = names_1981.reindex(names_1881.index)\n",
        "\n",
        "# Print shape of common_names\n",
        "print(common_names.shape)\n",
        "\"\"\"(1935, 1)\"\"\"\n",
        "print(common_names.head())\n",
        "\"\"\"\"\n",
        "    name      gender         \n",
        "    Mary      F       11030.0\n",
        "    Anna      F        5182.0\n",
        "    Emma      F         532.0\n",
        "    Elizabeth F       20168.0\n",
        "    Margaret  F        2791.0\"\"\"\"\n",
        "# Drop rows with null counts: common_names\n",
        "common_names = common_names.dropna()\n",
        "\n",
        "# Print shape of new common_names\n",
        "print(common_names.shape)\n",
        "\"\"\"(1587, 1)\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9FKos3GJkhZ",
        "colab_type": "text"
      },
      "source": [
        "#Importing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFa-Vhemu2OL",
        "colab_type": "text"
      },
      "source": [
        "## import a df with multilevel index from csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdDMIj5tu8Ib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "#Both have multi-index generated with the following:\n",
        "names_1981 = pd.read_csv('names1981.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
        "names_1881 = pd.read_csv('names1881.csv', header=None, names=['name','gender','count'], index_col=(0,1))\n",
        "print(names_1981.head())\n",
        "\"\"\"\n",
        "                 count\n",
        "    name     gender       \n",
        "    Jennifer F       57032\n",
        "    Jessica  F       42519\n",
        "    Amanda   F       34370\n",
        "    Sarah    F       28162\n",
        "    Melissa  F       28003\"\"\"\n",
        "print(names_1881.head())\n",
        "\"\"\"\n",
        "    name      gender       \n",
        "    Mary      F        6919\n",
        "    Anna      F        2698\n",
        "    Emma      F        2034\n",
        "    Elizabeth F        1852\n",
        "    Margaret  F        1658\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzdCE-L7Eu4h",
        "colab_type": "text"
      },
      "source": [
        "## Reading & Writing CSV\n",
        "also some manipulation via grouping and feature engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exD6wjMSEzhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From previous steps\n",
        "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
        "print(airline_bumping.head())\n",
        "airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n",
        "airline_totals[\"bumps_per_10k\"] = airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"] * 10000\n",
        "\n",
        "# Print airline_totals\n",
        "print(airline_totals)\n",
        "\"\"\"\n",
        "                         nb_bumped  total_passengers  bumps_per_10k\n",
        "    airline                                                        \n",
        "    ALASKA AIRLINES           1392          36543121          0.381\n",
        "    AMERICAN AIRLINES        11115         197365225          0.563\n",
        "    DELTA AIR LINES           1591         197033215          0.081\n",
        "    EXPRESSJET AIRLINES       3326          27858678          1.194\n",
        "    FRONTIER AIRLINES         1228          22954995          0.535\n",
        "    HAWAIIAN AIRLINES          122          16577572          0.074\n",
        "    JETBLUE AIRWAYS           3615          53245866          0.679\n",
        "    SKYWEST AIRLINES          3094          47091737          0.657\n",
        "    SOUTHWEST AIRLINES       18585         228142036          0.815\n",
        "    SPIRIT AIRLINES           2920          32304571          0.904\n",
        "    UNITED AIRLINES           4941         134468897          0.367\n",
        "    VIRGIN AMERICA             242          12017967          0.201\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eNcUi4VFqR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create airline_totals_sorted\n",
        "airline_totals_sorted = airline_totals.sort_values(\"bumps_per_10k\", ascending=False)\n",
        "\n",
        "# Print airline_totals_sorted\n",
        "print(airline_totals_sorted)\n",
        "\"\"\"\n",
        "                         nb_bumped  total_passengers  bumps_per_10k\n",
        "    airline                                                        \n",
        "    EXPRESSJET AIRLINES       3326          27858678          1.194\n",
        "    SPIRIT AIRLINES           2920          32304571          0.904\n",
        "    SOUTHWEST AIRLINES       18585         228142036          0.815\n",
        "    JETBLUE AIRWAYS           3615          53245866          0.679\n",
        "    SKYWEST AIRLINES          3094          47091737          0.657\n",
        "    AMERICAN AIRLINES        11115         197365225          0.563\n",
        "    FRONTIER AIRLINES         1228          22954995          0.535\n",
        "    ALASKA AIRLINES           1392          36543121          0.381\n",
        "    UNITED AIRLINES           4941         134468897          0.367\n",
        "    VIRGIN AMERICA             242          12017967          0.201\n",
        "    DELTA AIR LINES           1591         197033215          0.081\n",
        "    HAWAIIAN AIRLINES          122          16577572          0.074\"\"\"\n",
        "\n",
        "# Save as airline_totals_sorted.csv\n",
        "airline_totals_sorted.to_csv(\"airline_totals_sorted.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCZ3QmH-JDRJ",
        "colab_type": "text"
      },
      "source": [
        "##Read CSV and Parse Date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmsCeje1JIe0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monthly = pd.read_csv(\"datasets/monthly_deaths.csv\", parse_dates=[\"date\"])\n",
        "\"\"\"\n",
        "         date  births  deaths\n",
        "0  1841-01-01     254      37\n",
        "1  1841-02-01     239      18\n",
        "2  1841-03-01     277      12\n",
        "3  1841-04-01     255       4\n",
        "4  1841-05-01     255       2\n",
        "5  1841-06-01     200      10\n",
        "6  1841-07-01     190      16\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Yy5Xdi3qJOv",
        "colab_type": "text"
      },
      "source": [
        "##Read multiple csv files using a loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfLq01x2qM4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Create the list of file names: filenames\n",
        "filenames = ['Gold.csv', 'Silver.csv', 'Bronze.csv']\n",
        "\n",
        "# Create the list of three DataFrames: dataframes\n",
        "dataframes = []\n",
        "for filename in filenames:\n",
        "    dataframes.append(pd.read_csv(filename))\n",
        "\n",
        "# Print top 5 rows of 1st DataFrame in dataframes\n",
        "print(dataframes[0].head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKhZGqFMKDq7",
        "colab_type": "text"
      },
      "source": [
        "##USING A FOR LOOP TO RENAME CSV FILES FOR IMPORTING AND CONCATING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEmDxL2VKJEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize an empyy list: medals\n",
        "medals =[]\n",
        "medal_types = [\"bronze\", \"silver\", \"gold\"]\n",
        "\n",
        "for medal in medal_types:\n",
        "    # Create the file name: file_name\n",
        "    file_name = \"%s_top5.csv\" % medal\n",
        "    # Create list of column names: columns\n",
        "    columns = ['Country', medal]\n",
        "    # Read file_name into a DataFrame: medal_df\n",
        "    medal_df = pd.read_csv(file_name, header=0, index_col='Country', names=columns)\n",
        "    # Append medal_df to medals\n",
        "    medals.append(medal_df)\n",
        "\n",
        "# Concatenate medals horizontally: medals_df\n",
        "medals_df = pd.concat(medals, axis='columns')\n",
        "\n",
        "\"\"\"                    bronze  silver    gold\n",
        "    France           475.0   461.0     NaN\n",
        "    Germany          454.0     NaN   407.0\n",
        "    Italy              NaN   394.0   460.0\n",
        "    Soviet Union     584.0   627.0   838.0\n",
        "    United Kingdom   505.0   591.0   498.0\n",
        "    United States   1052.0  1195.0  2088.0\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A2pl6Rg4bL5",
        "colab_type": "text"
      },
      "source": [
        "##USING_CHUNKS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3J2NTCk74aWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#chunking the CPS file to get only unique ids and columns of interest\n",
        "cps_cols_to_keep = ['TUCASEID', 'TULINENO', 'HUFAMINC', 'HEFAMINC', 'HRYEAR4']\n",
        "\n",
        "#create the iterator\n",
        "cps_iter = pd.read_csv('data/atuscps_0318.dat', iterator= True, chunksize=20000, usecols=cps_cols_to_keep)\n",
        "\n",
        "#list for storing the dfs\n",
        "cps_list = []\n",
        "\n",
        "\n",
        "#iterator\n",
        "for chunk in cps_iter:\n",
        "    cps_temp = chunk[(chunk.TULINENO == 1) & (chunk.HRYEAR4 >= 2008)]\n",
        "    cps_temp = cps_temp.rename(columns={'TUCASEID':'case_id','HUFAMINC':'faminc_1','HEFAMINC':'faminc_2','HRYEAR4':'year'})\n",
        "    cps_list.append(cps_temp)\n",
        "\n",
        "cps_final = pd.concat(cps_list)\n",
        "cps_final.reset_index(drop=True, inplace=True)\n",
        "cps_final.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SWo86_aKnj6",
        "colab_type": "text"
      },
      "source": [
        "#DATETIME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "notF1C6U1qZp",
        "colab_type": "text"
      },
      "source": [
        "##Resampling date indexed data with .resample()\n",
        "\n",
        "uses an aggrigator to resample dates\n",
        ".sum()\n",
        ".last()\n",
        "requires DateTime data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AstXEHj917WY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read 'GDP.csv' into a DataFrame: gdp\n",
        "gdp = pd.read_csv(\"GDP.csv\", parse_dates=True, index_col='DATE')\n",
        "\n",
        "# Slice all the gdp data from 2008 onward: post2008\n",
        "post2008 = gdp.loc[\"2008\":]\n",
        "\n",
        "# Print the last 8 rows of post2008\n",
        "print(post2008.tail(8))\n",
        "\"\"\"   \n",
        "     DATE               \n",
        "    2014-07-01  17569.4\n",
        "    2014-10-01  17692.2\n",
        "    2015-01-01  17783.6\n",
        "    2015-04-01  17998.3\n",
        "    2015-07-01  18141.9\n",
        "    2015-10-01  18222.8\n",
        "    2016-01-01  18281.6\n",
        "    2016-04-01  18436.5\n",
        "\"\"\"\n",
        "# Resample post2008 by year, keeping last(): yearly\n",
        "yearly = post2008.resample('A').last() #last takes the last data point. sum would have summed all data for the year\n",
        "#print(yearly.tail())\n",
        "# Print yearly\n",
        "print(yearly)\n",
        "\"\"\"\n",
        "DATE               \n",
        "    2008-12-31  14549.9\n",
        "    2009-12-31  14566.5\n",
        "    2010-12-31  15230.2\n",
        "    2011-12-31  15785.3\n",
        "    2012-12-31  16297.3\n",
        "    2013-12-31  16999.9\n",
        "    2014-12-31  17692.2\n",
        "    2015-12-31  18222.8\n",
        "    2016-12-31  18436.5\n",
        "\"\"\"\n",
        "# Compute percentage growth of yearly: yearly['growth']\n",
        "yearly['growth'] = yearly.pct_change()*100\n",
        "\"\"\"\n",
        "DATE               \n",
        "    2008-12-31  14549.9\n",
        "    2009-12-31  14566.5\n",
        "    2010-12-31  15230.2\n",
        "    2011-12-31  15785.3\n",
        "    2012-12-31  16297.3\n",
        "    2013-12-31  16999.9\n",
        "    2014-12-31  17692.2\n",
        "    2015-12-31  18222.8\n",
        "    2016-12-31  18436.5\n",
        "\"\"\"\n",
        "# Print yearly again\n",
        "print(yearly)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etywiHXoxdbG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check individual values for missing values\n",
        "print(avocados_2016.isna())\n",
        "\"\"\"\n",
        "        date  avg_price  total_sold  small_sold  large_sold  xl_sold  total_bags_sold  small_bags_sold  large_bags_sold  xl_bags_sold\n",
        "    0   False      False       False       False       False    False            False            False            False         False\n",
        "    1   False      False       False       False       False    False            False            False            False         False\n",
        "    2   False      False       False       False        True    False            False            False            False         False\n",
        "    3   False      False       False       False       False    False            False            False            False         False\n",
        "    4   False      False       False       False       False     True            False            False            False         False\n",
        "    5   False      False       False        True       False    False            False            False            False         False\n",
        "    6   False      False       False       False       False    False            False            False            False         False\n",
        "    7   False      False       False       False        True    False            False            False            False         False\n",
        "    8   False      False       False       False       False    False            False            False            False         False\n",
        "    9   False      False       False       False       False    False            False            False            False         False\n",
        "    10  False      False       False       False        True    False            False            False            False         False\n",
        "    11  False      False       False       False       False    False            False            False            False         False\n",
        "    12  False      False       False       False       False    False            False            False            False         False\n",
        "    13  False      False       False       False       False    False            False            False            False         False\n",
        "    14  False      False       False       False       False    False            False            False            False         False\n",
        "\n",
        "\"\"\"\n",
        "# Check each column for missing values\n",
        "print(avocados_2016.isna().any())\n",
        "\"\"\"\n",
        "  date               False\n",
        "    avg_price          False\n",
        "    total_sold         False\n",
        "    small_sold          True\n",
        "    large_sold          True\n",
        "    xl_sold             True\n",
        "    total_bags_sold    False\n",
        "    small_bags_sold    False\n",
        "    large_bags_sold    False\n",
        "    xl_bags_sold       False\n",
        "    dtype: bool\n",
        "\"\"\"\n",
        "\n",
        "# Bar plot of missing values by variable\n",
        "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUuM505TyphS",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1zmy5LsZR9xuyjcjZsZnH4L2R5l0aXtHM)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpIiAXt1y0No",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove rows with missing values\n",
        "avocados_complete = avocados_2016.dropna()\n",
        "\n",
        "# Check if any columns contain missing values\n",
        "print(avocados_complete.isna().any())\n",
        "\"\"\"\n",
        "    date               False\n",
        "    avg_price          False\n",
        "    total_sold         False\n",
        "    small_sold         False\n",
        "    large_sold         False\n",
        "    xl_sold            False\n",
        "    total_bags_sold    False\n",
        "    small_bags_sold    False\n",
        "    large_bags_sold    False\n",
        "    xl_bags_sold       False\n",
        "    dtype: bool\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJgGLktl0IHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# From previous step\n",
        "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
        "avocados_2016[cols_with_missing].hist()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cTw9PVz0OpX",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=1bT7gw0XrzL6U7yGz1Y7_fX84Jmlkg5RB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCCsh7pk4OiE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fill in missing values with 0\n",
        "avocados_filled = avocados_2016.fillna(0)\n",
        "\n",
        "# Create histograms of the filled columns\n",
        "avocados_filled[cols_with_missing].hist()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JCAwIs4-gzh",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://drive.google.com/uc?id=11m6WBoCsAqmZQWhyQKrgw4zUEsl25_fW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtiSi8Dxqywu",
        "colab_type": "text"
      },
      "source": [
        "##SKYE PANDAS SLICING BY DATE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJCh3-FJq3Hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
        "print(temperatures[(temperatures[\"date\"] >= \"2010\") & (temperatures[\"date\"] < \"2012\")])\n",
        "\"\"\"\n",
        "             date     city        country  avg_temp_c\n",
        "    120   2010-01-01  Abidjan  Côte D'Ivoire      28.270\n",
        "    121   2010-02-01  Abidjan  Côte D'Ivoire      29.262\n",
        "    122   2010-03-01  Abidjan  Côte D'Ivoire      29.596\n",
        "    123   2010-04-01  Abidjan  Côte D'Ivoire      29.068\n",
        "    124   2010-05-01  Abidjan  Côte D'Ivoire      28.258\n",
        "    ...          ...      ...            ...         ...\n",
        "    16474 2011-08-01     Xian          China      23.069\n",
        "    16475 2011-09-01     Xian          China      16.775\n",
        "    16476 2011-10-01     Xian          China      12.587\n",
        "    16477 2011-11-01     Xian          China       7.543\n",
        "    16478 2011-12-01     Xian          China      -0.490\n",
        "    \n",
        "    [2400 rows x 4 columns]\"\"\"\n",
        "# Set date as an index\n",
        "temperatures_ind = temperatures.set_index(\"date\")\n",
        "\n",
        "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
        "print(temperatures_ind.loc[\"2010\":\"2011\"])\n",
        "\"\"\"\n",
        "               city        country  avg_temp_c\n",
        "    date                                          \n",
        "    2010-01-01  Abidjan  Côte D'Ivoire      28.270\n",
        "    2010-02-01  Abidjan  Côte D'Ivoire      29.262\n",
        "    2010-03-01  Abidjan  Côte D'Ivoire      29.596\n",
        "    2010-04-01  Abidjan  Côte D'Ivoire      29.068\n",
        "    2010-05-01  Abidjan  Côte D'Ivoire      28.258\n",
        "    ...             ...            ...         ...\n",
        "    2011-08-01     Xian          China      23.069\n",
        "    2011-09-01     Xian          China      16.775\n",
        "    2011-10-01     Xian          China      12.587\n",
        "    2011-11-01     Xian          China       7.543\n",
        "    2011-12-01     Xian          China      -0.490\n",
        "    \n",
        "    [2400 rows x 3 columns]\"\"\"\n",
        "\n",
        "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
        "print(temperatures_ind.loc[\"2010-08\":\"2011-02\"])\n",
        "\"\"\"\n",
        "               city        country  avg_temp_c\n",
        "    date                                          \n",
        "    2010-01-01  Abidjan  Côte D'Ivoire      28.270\n",
        "    2010-02-01  Abidjan  Côte D'Ivoire      29.262\n",
        "    2010-03-01  Abidjan  Côte D'Ivoire      29.596\n",
        "    2010-04-01  Abidjan  Côte D'Ivoire      29.068\n",
        "    2010-05-01  Abidjan  Côte D'Ivoire      28.258\n",
        "    ...             ...            ...         ...\n",
        "    2011-08-01     Xian          China      23.069\n",
        "    2011-09-01     Xian          China      16.775\n",
        "    2011-10-01     Xian          China      12.587\n",
        "    2011-11-01     Xian          China       7.543\n",
        "    2011-12-01     Xian          China      -0.490\n",
        "    \n",
        "    [2400 rows x 3 columns]\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF26A0NisG53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get 23rd row, 2nd column (index 22, 1)\n",
        "print(temperatures.iloc[22:23, 1:2])\n",
        "\n",
        "# Use slicing to get the first 5 rows\n",
        "print(temperatures.iloc[:6,:])\n",
        "\n",
        "# Use slicing to get columns 3 to 4\n",
        "print(temperatures.iloc[:,2:5])\n",
        "\n",
        "# Use slicing in both directions at once\n",
        "print(temperatures.iloc[:5,2:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV3HzugAL0nU",
        "colab_type": "text"
      },
      "source": [
        "##Using Date to split data\n",
        "pd.to_datetime()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4Hd3IWCL6PP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "handwashing_start = pd.to_datetime('1847-06-01')\n",
        "\n",
        "# Split monthly into before and after handwashing_start\n",
        "before_washing = monthly[monthly[\"date\"]<handwashing_start]\n",
        "after_washing = monthly[monthly[\"date\"]>=handwashing_start]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ziUIx-ZKeDH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYKDbygxw-5C",
        "colab_type": "text"
      },
      "source": [
        "#Mathmatical opperations across dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYjrbgTgxG53",
        "colab_type": "text"
      },
      "source": [
        "##simple math across a single dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvmQMxcFxGFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract selected columns from weather as new DataFrame: temps_f\n",
        "temps_f = weather[['Min TemperatureF','Mean TemperatureF','Max TemperatureF']]\n",
        "\n",
        "# Convert temps_f to celsius: temps_c\n",
        "temps_c = (temps_f-32)* 5/9\n",
        "\n",
        "# Rename 'F' in column names with 'C': temps_c.columns\n",
        "temps_c.columns = temps_c.columns.str.replace('F','C')\n",
        "\n",
        "# Print first 5 rows of temps_c\n",
        "print(temps_c.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FX7JgQa32g_F",
        "colab_type": "text"
      },
      "source": [
        "## calculating percent change using .pct_change()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WciW896E2f5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read 'GDP.csv' into a DataFrame: gdp\n",
        "gdp = pd.read_csv(\"GDP.csv\", parse_dates=True, index_col='DATE')\n",
        "\n",
        "# Slice all the gdp data from 2008 onward: post2008\n",
        "post2008 = gdp.loc[\"2008\":]\n",
        "\n",
        "# Print the last 8 rows of post2008\n",
        "print(post2008.tail(8))\n",
        "\"\"\"   \n",
        "     DATE               \n",
        "    2014-07-01  17569.4\n",
        "    2014-10-01  17692.2\n",
        "    2015-01-01  17783.6\n",
        "    2015-04-01  17998.3\n",
        "    2015-07-01  18141.9\n",
        "    2015-10-01  18222.8\n",
        "    2016-01-01  18281.6\n",
        "    2016-04-01  18436.5\n",
        "\"\"\"\n",
        "# Resample post2008 by year, keeping last(): yearly\n",
        "yearly = post2008.resample('A').last() #last takes the last data point. sum would have summed all data for the year\n",
        "#print(yearly.tail())\n",
        "# Print yearly\n",
        "print(yearly)\n",
        "\"\"\"\n",
        "DATE               \n",
        "    2008-12-31  14549.9\n",
        "    2009-12-31  14566.5\n",
        "    2010-12-31  15230.2\n",
        "    2011-12-31  15785.3\n",
        "    2012-12-31  16297.3\n",
        "    2013-12-31  16999.9\n",
        "    2014-12-31  17692.2\n",
        "    2015-12-31  18222.8\n",
        "    2016-12-31  18436.5\n",
        "\"\"\"\n",
        "# Compute percentage growth of yearly: yearly['growth']\n",
        "yearly['growth'] = yearly.pct_change()*100\n",
        "\"\"\"\n",
        "DATE               \n",
        "    2008-12-31  14549.9\n",
        "    2009-12-31  14566.5\n",
        "    2010-12-31  15230.2\n",
        "    2011-12-31  15785.3\n",
        "    2012-12-31  16297.3\n",
        "    2013-12-31  16999.9\n",
        "    2014-12-31  17692.2\n",
        "    2015-12-31  18222.8\n",
        "    2016-12-31  18436.5\n",
        "\"\"\"\n",
        "# Print yearly again\n",
        "print(yearly)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryOXM5QD3oLv",
        "colab_type": "text"
      },
      "source": [
        "## muliplying across df with another df using .multiply()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KU6Zr_Ff3ybM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Read 'sp500.csv' into a DataFrame: sp500\n",
        "sp500 = pd.read_csv(\"sp500.csv\", parse_dates=True, index_col='Date')\n",
        "\n",
        "# Read 'exchange.csv' into a DataFrame: exchange\n",
        "exchange = pd.read_csv(\"exchange.csv\", parse_dates=True, index_col='Date')\n",
        "\n",
        "# Subset 'Open' & 'Close' columns from sp500: dollars\n",
        "dollars = sp500[[\"Open\",\"Close\"]]\n",
        "\n",
        "# Print the head of dollars\n",
        "print(dollars.head())\n",
        "\"\"\"\n",
        "                   Open        Close\n",
        "    Date                                \n",
        "    2015-01-02  2058.899902  2058.199951\n",
        "    2015-01-05  2054.439941  2020.579956\n",
        "    2015-01-06  2022.150024  2002.609985\n",
        "    2015-01-07  2005.550049  2025.900024\n",
        "    2015-01-08  2030.609985  2062.139893\n",
        "                       Open        Close\"\"\"\n",
        "# Convert dollars to pounds: pounds\n",
        "pounds = dollars.multiply(exchange[\"GBP/USD\"], axis=\"rows\")\n",
        "\n",
        "# Print the head of pounds\n",
        "print(pounds.head())\n",
        "\"\"\"\n",
        "                   Open        Close\n",
        "    Date                                \n",
        "    2015-01-02  1340.364425  1339.908750\n",
        "    2015-01-05  1348.616555  1326.389506\n",
        "    2015-01-06  1332.515980  1319.639876\n",
        "    2015-01-07  1330.562125  1344.063112\n",
        "    2015-01-08  1343.268811  1364.126161\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9HHadaVFI01",
        "colab_type": "text"
      },
      "source": [
        "## COUNT THE MAX NUMBER OF A COLUMN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VTyexp9FOHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "most_songs = int(max(no_bands['num_songs'].values))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9_8Zn6eFLOw",
        "colab_type": "text"
      },
      "source": [
        "# Contatenating APPEND MERGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVoQnrmJFRW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize empty list: units\n",
        "units = []\n",
        "\n",
        "# Build the list of Series\n",
        "for month in [jan, feb, mar]:\n",
        "    units.append(month['Units'])\n",
        "\n",
        "# Concatenate the list: quarter1\n",
        "quarter1 = pd.concat(units, axis='rows')\n",
        "\n",
        "# Print slices from quarter1\n",
        "print(quarter1.loc['jan 27, 2015':'feb 2, 2015'])\n",
        "print(quarter1.loc['feb 26, 2015':'mar 7, 2015'])\n",
        "\"\"\"\n",
        "    Date\n",
        "    2015-01-27 07:11:55    18\n",
        "    2015-02-02 08:33:01     3\n",
        "    2015-02-02 20:54:49     9\n",
        "    Name: Units, dtype: int64\n",
        "    Date\n",
        "    2015-02-26 08:57:45     4\n",
        "    2015-02-26 08:58:51     1\n",
        "    2015-03-06 10:11:45    17\n",
        "    2015-03-06 02:03:56    17\n",
        "    Name: Units, dtype: int64\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-eHQ7wtHIeB",
        "colab_type": "text"
      },
      "source": [
        "## combining dfs along the row axis with 'ignore index\" using append()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-wkLdhxHPKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add 'year' column to names_1881 and names_1981\n",
        "names_1881['year'] = 1881\n",
        "names_1981['year'] = 1981\n",
        "\n",
        "# Append names_1981 after names_1881 with ignore_index=True: combined_names\n",
        "combined_names = names_1881.append(names_1981, ignore_index=True)\n",
        "\n",
        "# Print shapes of names_1981, names_1881, and combined_names\n",
        "print(names_1981.shape)\n",
        "print(names_1881.shape)\n",
        "print(combined_names.shape)\n",
        "\"\"\"\n",
        "    (19455, 4)\n",
        "    (1935, 4)\n",
        "    (21390, 4)\"\"\"\n",
        "\n",
        "# Print all rows that contain the name 'Morgan'\n",
        "print(combined_names[combined_names[\"name\"]== \"Morgan\"])\n",
        "\"\"\"\n",
        "             name gender  count  year\n",
        "    1283   Morgan      M     23  1881\n",
        "    2096   Morgan      F   1769  1981\n",
        "    14390  Morgan      M    766  1981\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UHxT75rH14Z",
        "colab_type": "text"
      },
      "source": [
        "##Concatenating dfs along the column axis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFfmeFNGH8nZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a list of weather_max and weather_mean\n",
        "weather_list = [weather_max,weather_mean]\n",
        "\n",
        "# Concatenate weather_list horizontally\n",
        "weather = pd.concat(weather_list, axis=1)\n",
        "\n",
        "# Print weather\n",
        "print(weather)\n",
        "\"\"\"\n",
        "      Max TemperatureF  Mean TemperatureF\n",
        "    Apr              89.0          53.100000\n",
        "    Aug               NaN          70.000000\n",
        "    Dec               NaN          34.935484\n",
        "    Feb               NaN          28.714286\n",
        "    Jan              68.0          32.354839\n",
        "    Jul              91.0          72.870968\n",
        "    Jun               NaN          70.133333\n",
        "    Mar               NaN          35.000000\n",
        "    May               NaN          62.612903\n",
        "    Nov               NaN          39.800000\n",
        "    Oct              84.0          55.451613\n",
        "    Sep               NaN          63.766667\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXmnXtbAJG3P",
        "colab_type": "text"
      },
      "source": [
        "##Concatenating over multiple csv files along the column axis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fNBohPyJGmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Initialize an empyy list: medals\n",
        "medals =[]\n",
        "#medal_types = [\"bronze\", \"silver\", \"gold\"]\n",
        "\n",
        "for medal in medal_types:\n",
        "    # Create the file name: file_name\n",
        "    file_name = \"%s_top5.csv\" % medal\n",
        "    # Create list of column names: columns\n",
        "    columns = ['Country', medal]\n",
        "    # Read file_name into a DataFrame: medal_df\n",
        "    medal_df = pd.read_csv(file_name, header=0, index_col='Country', names=columns)\n",
        "    # Append medal_df to medals\n",
        "    medals.append(medal_df)\n",
        "\n",
        "# Concatenate medals horizontally: medals_df\n",
        "medals_df = pd.concat(medals, axis='columns')\n",
        "\"\"\"\n",
        "                    bronze  silver    gold\n",
        "    France           475.0   461.0     NaN\n",
        "    Germany          454.0     NaN   407.0\n",
        "    Italy              NaN   394.0   460.0\n",
        "    Soviet Union     584.0   627.0   838.0\n",
        "    United Kingdom   505.0   591.0   498.0\n",
        "    United States   1052.0  1195.0  2088.0\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMEmunTD_Rxa",
        "colab_type": "text"
      },
      "source": [
        "##SKYE_PANDAS_Joining_two_DFs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iIMg8sy_RJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " merge = pd.merge(df,error, left_on=['series_id', 'year'], right_on=['series_id', 'year'], how='left').reset_index(drop=True).copy(deep=True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}